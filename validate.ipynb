{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install mediapipe\n",
    "# !git config --global http.sslVerify false\n",
    "# !pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "origin_data = glob.glob(r\"archive/Videos/*\")\n",
    "print(len(origin_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "origin_data_violent = glob.glob(r\"archive/Videos/Violent_*\")\n",
    "origin_data_normal = glob.glob(r\"archive/Videos/Normal_*\")\n",
    "print(len(origin_data_violent))\n",
    "print(len(origin_data_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, Dense,Dropout, GlobalAveragePooling2D, GlobalAveragePooling1D\n",
    "from keras.models import Sequential, load_model\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "import torch\n",
    "from threading import Thread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5n')\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy keypoint 1 video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_global = 0\n",
    "def EachVideo(linkVideo): # This function is used to cycle through a video\n",
    "    X = []\n",
    "    video_path =r\"\"+ linkVideo  # The path to the data file I use\n",
    "    print(f\"playing : {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    skipTime  = 0\n",
    "    skipFrame = 0\n",
    "    while True:    \n",
    "        ret, frame = cap.read()\n",
    "        skipTime = skipTime +1\n",
    "        if not ret:\n",
    "            break\n",
    "        if 1==1:\n",
    "#         if skipTime >= 30: # When skipTime has passed the first 30 frames, ie the first 1 second, proceed to Detect Person\n",
    "            skipFrame = skipFrame +1 # The variable skipFrame means that every 5 frames I will detect 1 time, so in 1 second I will detect 6 times\n",
    "            # print(skipFrame)\n",
    "#             if  skipFrame  == 5: # When skipFrame = 5, I will detect person and assign skipFrame = 0 to run again\n",
    "            if 1==1:\n",
    "                skipFrame = 0\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame.flags.writeable = False  \n",
    "                result = yolo_model(frame)     # Detect Person\n",
    "                frame.flags.writeable = True   \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                for (xmin, ymin, xmax,   ymax,  confidence,  clas) in result.xyxy[0].tolist(): # Loop through all the Persons present in the video, giving the x,y of each Person\n",
    "                    c_lm = []\n",
    "                    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "                            \n",
    "#                             frame.flags.writeable = False  \n",
    "\n",
    "                            resulta = pose.process(frame[int(ymin):int(ymax),int(xmin):int(xmax):])\n",
    "#                             frame.flags.writeable = True   \n",
    "\n",
    "                            if resulta.pose_landmarks and clas ==0: # class here is class, class = 0 means human\n",
    "                                for (id, lm) in enumerate(resulta.pose_landmarks.landmark):\n",
    "                                    if id > 10 and id not in [17,18,19,20,21,22] and id not in [29,30,31,32] :\n",
    "                                        c_lm.append(lm.x)\n",
    "                                        c_lm.append(lm.y)\n",
    "#                                         c_lm.append(lm.z)\n",
    "#                                         c_lm.append(lm.visibility)\n",
    "                    if len(c_lm) > 0: # c_ lm used to save a person's x and y variables in a loop through each person, when saving, there will be a state that there is no x,y data to save \n",
    "                        X.append(c_lm) # with linkVideo being violent, we add data to X_violent\n",
    "#                 if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                       break\n",
    "    return X\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through multiple videos to get keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AllVideo(startt,endd,dataset): # This function is used to cycle through all videos\n",
    "# Status (linkVideo) is : violent, non-violent\n",
    "    X = []\n",
    "    \n",
    "# cam (camIndex) is : cam1 or cam2\n",
    "# number of videos is : from startt to endd, for example non-violent dataset is startt 1->61 with cam 1, violent from 1 to 115 with cam 1\n",
    "    for id,i in enumerate(range(startt,endd)):\n",
    "        print(f'{startt} -> {endd} ## index: {startt+id}')\n",
    "        X.extend(EachVideo(dataset[i])) # Implement the function to detect\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save keypoint, run getData function to run multiple videos at the same time (threading) to get keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "def getData(startt,endd,origin_data):\n",
    "    global X\n",
    "    X = AllVideo(startt,endd,origin_data)\n",
    "\n",
    "    \n",
    "# active = []\n",
    "# skipT = 1500\n",
    "# for i in range(0,10):\n",
    "#     active_1 = Thread(target=getData,args=(skipT,skipT+20,origin_data_normal))\n",
    "#     skipT = skipT + 20\n",
    "#     active.append(active_1)\n",
    "\n",
    "# for i in active:\n",
    "#     i.start()\n",
    "# for i in active:\n",
    "#     i.join()\n",
    "\n",
    "getData(1500, 1700, origin_data_normal)\n",
    "\n",
    "print(len(X))\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "# X = []\n",
    "# def getData(startt,endd,origin_data):\n",
    "#     global X\n",
    "#     X = AllVideo(startt,endd,origin_data)\n",
    "\n",
    "    \n",
    "# active = []\n",
    "# skipT = 0\n",
    "# for i in range(0,10):\n",
    "#     active_1 = Thread(target=getData,args=(skipT,skipT+20,origin_data_violent))\n",
    "#     skipT = skipT + 20\n",
    "#     active.append(active_1)\n",
    "\n",
    "# for i in active:\n",
    "#     i.start()\n",
    "# for i in active:\n",
    "#     i.join()\n",
    "    \n",
    "# print(len(X))\n",
    "# X = np.array(X)\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "Xt = np.array(X)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save keypoint to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = pd.DataFrame(Xt)\n",
    "Xt  = np.nan_to_num(Xt) # Khi đã có dữ liệu, sẽ có một số phần từ trong dữ liệu bị NaN, nên chuyển thành 0\n",
    "Xt = pd.DataFrame(Xt)\n",
    "Xt.to_csv(\"./normal_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Crane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violent_test = pd.read_csv(\"voilent_0_650_final.csv\")\n",
    "\n",
    "normal_test = pd.read_csv(\"normal_test.csv\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# X_violent = violent_test.iloc[:,1:].values\n",
    "# len_X_violent = len(X_violent)\n",
    "# for i in range(10,len_X_violent):\n",
    "#     X.append(X_violent[i-10:i,:])\n",
    "#     y.append(1)\n",
    "    \n",
    "    \n",
    "X_non_violent = normal_test.iloc[:,1:].values\n",
    "len_X_non_violent = len(X_non_violent)\n",
    "for i in range(10,len_X_non_violent):\n",
    "    X.append(X_non_violent[i-10:i,:])\n",
    "    y.append(0)\n",
    "    \n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)    \n",
    "# print(violent_test.shape)\n",
    "print(normal_test.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/first.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the TEST dataset\n",
    "loss, accuracy = model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X[0].reshape(-1, 10, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "model = load_model(\"models/pb_model/\")\n",
    "\n",
    "def inference(linkVideo): # This function is used to cycle through a video\n",
    "    X = []\n",
    "    idx = 0\n",
    "    alert_count = 0\n",
    "    video_path = linkVideo  # The path to the data file I use\n",
    "    print(f\"playing : {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    size = (frame_width, frame_height)\n",
    "\n",
    "    # Below VideoWriter object will create\n",
    "    # a frame of above defined The output \n",
    "    # is stored in 'filename.avi' file.\n",
    "    result_writer = cv2.VideoWriter('filename.avi', \n",
    "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                             10, size)\n",
    "    skipTime  = 0\n",
    "    skipFrame = 0\n",
    "    while True:    \n",
    "        ret, frame = cap.read()\n",
    "        skipTime = skipTime +1\n",
    "        if not ret:\n",
    "            break\n",
    "        if 1==1:\n",
    "#         if skipTime >= 30: # When skipTime has passed the first 30 frames, ie the first 1 second, proceed to Detect Person\n",
    "            skipFrame = skipFrame +1 # The variable skipFrame means that every 5 frames I will detect 1 time, so in 1 second I will detect 6 times\n",
    "            # print(skipFrame)\n",
    "#             if  skipFrame  == 5: # When skipFrame = 5, I will detect person and assign skipFrame = 0 to run again\n",
    "            if 1==1:\n",
    "                skipFrame = 0\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame.flags.writeable = False  \n",
    "                result = yolo_model(frame)     # Detect Person\n",
    "                frame.flags.writeable = True   \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                for (xmin, ymin, xmax,   ymax,  confidence,  clas) in result.xyxy[0].tolist(): # Loop through all the Persons present in the video, giving the x,y of each Person\n",
    "                    c_lm = []\n",
    "                    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "                            \n",
    "#                             frame.flags.writeable = False  \n",
    "\n",
    "                            resulta = pose.process(frame[int(ymin):int(ymax),int(xmin):int(xmax):])\n",
    "#                             frame.flags.writeable = True   \n",
    "\n",
    "                            if resulta.pose_landmarks and clas == 0: # class here is class, class = 0 means human\n",
    "                                for (id, lm) in enumerate(resulta.pose_landmarks.landmark):\n",
    "                                    if id > 10 and id not in [17,18,19,20,21,22] and id not in [29,30,31,32] :\n",
    "                                        c_lm.append(lm.x)\n",
    "                                        c_lm.append(lm.y)\n",
    "#                                         c_lm.append(lm.z)\n",
    "#                                         c_lm.append(lm.visibility)\n",
    "                    if len(c_lm) > 0: # c_ lm used to save a person's x and y variables in a loop through each person, when saving, there will be a state that there is no x,y data to save \n",
    "                        X.append(c_lm) # with linkVideo being violent, we add data to X_violent\n",
    "                \n",
    "                    if len(X) >= idx+10:\n",
    "                        X_inp = np.array(X[idx:idx+10])\n",
    "                        pred = model.predict(X_inp.reshape(-1, 10, 24))\n",
    "                        print(pred[0][0])\n",
    "                        if pred[0][0] > 0.50:\n",
    "                            # cv2.putText(frame, str(pred[0][0]), (10, 10), cv2.FONT_HERSHEY_COMPLEX, 2,(255,255,255),3)\n",
    "                            alert_count += 1\n",
    "                        else:\n",
    "                            alert_count = 0\n",
    "                        idx +=1\n",
    "        if alert_count > 5:\n",
    "            cv2.putText(frame, \"Aggression Behaviour Detected!!!\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 2,(0,0,255),3)\n",
    "        \n",
    "        result_writer.write(frame)\n",
    "        cv2.imshow(\"pose\", frame)        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    cap.release()\n",
    "    result_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "video = 0#\"/Users/neo/Downloads/AB2.mp4\"\n",
    "\n",
    "inference(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[0.7699176073074341, 0.19481432437896729, 0.5611128807067871, 0.22867439687252045, 0.6571943759918213, 0.3317640423774719, 0.4230298697948456, 0.3323039412498474, 0.49991655349731445, 0.22421276569366455, 0.34415504336357117, 0.2337348759174347, 0.7032572627067566, 0.4559857249259949, 0.6076101064682007, 0.4639747738838196, 0.26415306329727173, 0.4007982015609741, 0.5575037002563477, 0.6610302329063416, 0.2519151568412781, 0.6272770166397095, 0.7672494053840637, 0.86176997423172], [0.8002002835273743, 0.18507182598114014, 0.6590595841407776, 0.21902433037757874, 0.7291820049285889, 0.3324247896671295, 0.5574362277984619, 0.3257876932621002, 0.5770357251167297, 0.23748062551021576, 0.5008056163787842, 0.25796061754226685, 0.763862669467926, 0.4382103681564331, 0.7009424567222595, 0.45691877603530884, 0.4155151844024658, 0.36088335514068604, 0.6360172629356384, 0.6626571416854858, 0.23995018005371094, 0.5761902332305908, 0.8091439008712769, 0.8645716905593872], [0.7171511650085449, 0.18791154026985168, 0.5591759085655212, 0.21526306867599487, 0.6272274255752563, 0.33659082651138306, 0.433452844619751, 0.33109766244888306, 0.3901101350784302, 0.2444138526916504, 0.3178172707557678, 0.2811466157436371, 0.6972371339797974, 0.4382787346839905, 0.6342501044273376, 0.4626223146915436, 0.28934618830680847, 0.34801188111305237, 0.5316084027290344, 0.6587507128715515, -0.025648117065429688, 0.4831083118915558, 0.7644127607345581, 0.8608119487762451], [0.6893287301063538, 0.19599391520023346, 0.5128780007362366, 0.21790853142738342, 0.6000369191169739, 0.34158289432525635, 0.3215474784374237, 0.3228086829185486, 0.3397133946418762, 0.2577214241027832, 0.21499738097190857, 0.2812578082084656, 0.6812333464622498, 0.43637919425964355, 0.5968048572540283, 0.4524202048778534, 0.21586015820503235, 0.3585386574268341, 0.5283343195915222, 0.6568543314933777, -0.1270851492881775, 0.46344268321990967, 0.7503399848937988, 0.8633987903594971], [0.6955459117889404, 0.20055142045021057, 0.5212627649307251, 0.21825098991394043, 0.549759566783905, 0.33697789907455444, 0.30090826749801636, 0.3168599009513855, 0.32415109872817993, 0.23848530650138855, 0.24736282229423523, 0.25915658473968506, 0.6371828317642212, 0.4478946924209595, 0.5727012753486633, 0.464826375246048, 0.24603480100631714, 0.33000892400741577, 0.5513866543769836, 0.6733344793319702, -0.10839593410491943, 0.42913538217544556, 0.7684546709060669, 0.8761069774627686], [0.7889373302459717, 0.2048453390598297, 0.6260793209075928, 0.22049082815647125, 0.6409932971000671, 0.3373645544052124, 0.4748993515968323, 0.31221434473991394, 0.5018362998962402, 0.2333289086818695, 0.4510005712509155, 0.25470805168151855, 0.7270090579986572, 0.4494051933288574, 0.6558668613433838, 0.46449020504951477, 0.3966815173625946, 0.33331865072250366, 0.6576957106590271, 0.670881986618042, 0.07779788970947266, 0.5056755542755127, 0.8354631662368774, 0.868228554725647], [0.7396079897880554, 0.19530317187309265, 0.5498781800270081, 0.21832185983657837, 0.5418987274169922, 0.324590802192688, 0.4135075509548187, 0.318592369556427, 0.3766399025917053, 0.21714730560779572, 0.33547070622444153, 0.2502281069755554, 0.6620655059814453, 0.4514329731464386, 0.5852506756782532, 0.46653875708580017, 0.25749480724334717, 0.34477782249450684, 0.5664358139038086, 0.6659743189811707, 0.021802246570587158, 0.54973965883255, 0.7913979887962341, 0.8634072542190552], [0.7037594318389893, 0.19721606373786926, 0.5027710199356079, 0.21063736081123352, 0.44926005601882935, 0.330055296421051, 0.3609119653701782, 0.3215879201889038, 0.28871551156044006, 0.2161647528409958, 0.27636072039604187, 0.2412261962890625, 0.6272782683372498, 0.4502488076686859, 0.5420613884925842, 0.45198962092399597, 0.13730564713478088, 0.37792515754699707, 0.5276956558227539, 0.6622517108917236, -0.016354084014892578, 0.5867432355880737, 0.7637906074523926, 0.8618974685668945], [0.5854372382164001, 0.1862086057662964, 0.32613736391067505, 0.22003135085105896, 0.5611717104911804, 0.2939192056655884, 0.11588329076766968, 0.2197476029396057, 0.3728986978530884, 0.20415303111076355, 0.18089723587036133, 0.09843826293945312, 0.5303086042404175, 0.4796312153339386, 0.4474417269229889, 0.4827785789966583, 0.3328123688697815, 0.6714822053909302, 0.576732337474823, 0.683230996131897, 0.2963668704032898, 0.9070543050765991, 0.8202749490737915, 0.8387964963912964]]\n",
    "lst.append(lst[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "la = np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 24)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 872ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(la.reshape(-1, 10, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23981388"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = {\"objects\":[{\"detection\":{\"bounding_box\":{\"x_max\":0.5107938051223755,\"x_min\":0.2977558374404907,\"y_max\":0.9771633446216583,\"y_min\":0.03176644444465637},\"confidence\":0.9937620759010315,\"label_id\":1},\"h\":446,\"id\":1,\"region_id\":275,\"tensors\":[{\"confidence\":0.9937620759010315,\"label_id\":1,\"layer_name\":\"detection_out\",\"layout\":\"ANY\",\"model_name\":\"person-vehicle-bike-detection-2002\",\"name\":\"detection\",\"precision\":\"UNSPECIFIED\"},{\"layout\":\"ANY\",\"name\":\"object_id\",\"precision\":\"UNSPECIFIED\"},{\"data\":[0.39270099997520447,0.10869564861059189,0.39270099997520447,0.08695652335882187,0.338535338640213,0.08695652335882187,0.5551979541778564,0.08695652335882187,0.501032292842865,0.08695652335882187,0.5551979541778564,0.15217392146587372,0.6635292768478394,0.15217392146587372,0.28436967730522156,0.260869562625885,0.44686663150787354,0.30434784293174744,0.17603838443756104,0.15217392146587372,0.28436967730522156,0.260869562625885,0.6093636155128479,0.4565217196941376,0.7176949381828308,0.4565217196941376,0.28436967730522156,0.6521739363670349,0.6635292768478394,0.6739130616188049,0.338535338640213,0.9130434393882751,0.9343575239181519,0.8695651888847351],\"dims\":[17,2],\"format\":\"keypoints\",\"layer_name\":\"heatmaps\",\"layout\":\"ANY\",\"model_name\":\"torch-jit-export\",\"name\":\"classification_layer_name:heatmaps\",\"point_connections\":[\"shoulder_l\",\"shoulder_r\",\"nose\",\"eye_l\",\"nose\",\"eye_r\",\"eye_l\",\"ear_l\",\"eye_r\",\"ear_r\",\"elbow_l\",\"shoulder_l\",\"elbow_r\",\"shoulder_r\",\"wrist_l\",\"elbow_l\",\"wrist_r\",\"elbow_r\",\"hip_l\",\"knee_l\",\"hip_r\",\"knee_r\",\"knee_l\",\"ankle_l\",\"knee_r\",\"ankle_r\"],\"point_names\":[\"nose\",\"eye_l\",\"eye_r\",\"ear_l\",\"ear_r\",\"shoulder_l\",\"shoulder_r\",\"elbow_l\",\"elbow_r\",\"wrist_l\",\"wrist_r\",\"hip_l\",\"hip_r\",\"knee_l\",\"knee_r\",\"ankle_l\",\"ankle_r\"],\"precision\":\"FP32\"}],\"w\":179,\"x\":250,\"y\":15}],\"resolution\":{\"height\":472,\"width\":840},\"timestamp\":800000000}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = json.dumps(st, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"objects\": [\n",
      "        {\n",
      "            \"detection\": {\n",
      "                \"bounding_box\": {\n",
      "                    \"x_max\": 0.5107938051223755,\n",
      "                    \"x_min\": 0.2977558374404907,\n",
      "                    \"y_max\": 0.9771633446216583,\n",
      "                    \"y_min\": 0.03176644444465637\n",
      "                },\n",
      "                \"confidence\": 0.9937620759010315,\n",
      "                \"label_id\": 1\n",
      "            },\n",
      "            \"h\": 446,\n",
      "            \"id\": 1,\n",
      "            \"region_id\": 275,\n",
      "            \"tensors\": [\n",
      "                {\n",
      "                    \"confidence\": 0.9937620759010315,\n",
      "                    \"label_id\": 1,\n",
      "                    \"layer_name\": \"detection_out\",\n",
      "                    \"layout\": \"ANY\",\n",
      "                    \"model_name\": \"person-vehicle-bike-detection-2002\",\n",
      "                    \"name\": \"detection\",\n",
      "                    \"precision\": \"UNSPECIFIED\"\n",
      "                },\n",
      "                {\n",
      "                    \"layout\": \"ANY\",\n",
      "                    \"name\": \"object_id\",\n",
      "                    \"precision\": \"UNSPECIFIED\"\n",
      "                },\n",
      "                {\n",
      "                    \"data\": [\n",
      "                        0.39270099997520447,\n",
      "                        0.10869564861059189,\n",
      "                        0.39270099997520447,\n",
      "                        0.08695652335882187,\n",
      "                        0.338535338640213,\n",
      "                        0.08695652335882187,\n",
      "                        0.5551979541778564,\n",
      "                        0.08695652335882187,\n",
      "                        0.501032292842865,\n",
      "                        0.08695652335882187,\n",
      "                        0.5551979541778564,\n",
      "                        0.15217392146587372,\n",
      "                        0.6635292768478394,\n",
      "                        0.15217392146587372,\n",
      "                        0.28436967730522156,\n",
      "                        0.260869562625885,\n",
      "                        0.44686663150787354,\n",
      "                        0.30434784293174744,\n",
      "                        0.17603838443756104,\n",
      "                        0.15217392146587372,\n",
      "                        0.28436967730522156,\n",
      "                        0.260869562625885,\n",
      "                        0.6093636155128479,\n",
      "                        0.4565217196941376,\n",
      "                        0.7176949381828308,\n",
      "                        0.4565217196941376,\n",
      "                        0.28436967730522156,\n",
      "                        0.6521739363670349,\n",
      "                        0.6635292768478394,\n",
      "                        0.6739130616188049,\n",
      "                        0.338535338640213,\n",
      "                        0.9130434393882751,\n",
      "                        0.9343575239181519,\n",
      "                        0.8695651888847351\n",
      "                    ],\n",
      "                    \"dims\": [\n",
      "                        17,\n",
      "                        2\n",
      "                    ],\n",
      "                    \"format\": \"keypoints\",\n",
      "                    \"layer_name\": \"heatmaps\",\n",
      "                    \"layout\": \"ANY\",\n",
      "                    \"model_name\": \"torch-jit-export\",\n",
      "                    \"name\": \"classification_layer_name:heatmaps\",\n",
      "                    \"point_connections\": [\n",
      "                        \"shoulder_l\",\n",
      "                        \"shoulder_r\",\n",
      "                        \"nose\",\n",
      "                        \"eye_l\",\n",
      "                        \"nose\",\n",
      "                        \"eye_r\",\n",
      "                        \"eye_l\",\n",
      "                        \"ear_l\",\n",
      "                        \"eye_r\",\n",
      "                        \"ear_r\",\n",
      "                        \"elbow_l\",\n",
      "                        \"shoulder_l\",\n",
      "                        \"elbow_r\",\n",
      "                        \"shoulder_r\",\n",
      "                        \"wrist_l\",\n",
      "                        \"elbow_l\",\n",
      "                        \"wrist_r\",\n",
      "                        \"elbow_r\",\n",
      "                        \"hip_l\",\n",
      "                        \"knee_l\",\n",
      "                        \"hip_r\",\n",
      "                        \"knee_r\",\n",
      "                        \"knee_l\",\n",
      "                        \"ankle_l\",\n",
      "                        \"knee_r\",\n",
      "                        \"ankle_r\"\n",
      "                    ],\n",
      "                    \"point_names\": [\n",
      "                        \"nose\",\n",
      "                        \"eye_l\",\n",
      "                        \"eye_r\",\n",
      "                        \"ear_l\",\n",
      "                        \"ear_r\",\n",
      "                        \"shoulder_l\",\n",
      "                        \"shoulder_r\",\n",
      "                        \"elbow_l\",\n",
      "                        \"elbow_r\",\n",
      "                        \"wrist_l\",\n",
      "                        \"wrist_r\",\n",
      "                        \"hip_l\",\n",
      "                        \"hip_r\",\n",
      "                        \"knee_l\",\n",
      "                        \"knee_r\",\n",
      "                        \"ankle_l\",\n",
      "                        \"ankle_r\"\n",
      "                    ],\n",
      "                    \"precision\": \"FP32\"\n",
      "                }\n",
      "            ],\n",
      "            \"w\": 179,\n",
      "            \"x\": 250,\n",
      "            \"y\": 15\n",
      "        }\n",
      "    ],\n",
      "    \"resolution\": {\n",
      "        \"height\": 472,\n",
      "        \"width\": 840\n",
      "    },\n",
      "    \"timestamp\": 800000000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data = pose_data[0]['tensors'][2]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_lm = [] \n",
    "for (id, lm) in enumerate(pose_data):\n",
    "    # print(id)\n",
    "    if id > 9:# and id not in [18,19,20,21,22] and id not in [29,30,31,32] :\n",
    "        c_lm.append(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5551979541778564"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_lm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'circle'\n> Overload resolution failed:\n>  - Can't parse 'center'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'center'. Sequence item with index 0 has a wrong type\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-e4ca222b3536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_lm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_lm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'circle'\n> Overload resolution failed:\n>  - Can't parse 'center'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'center'. Sequence item with index 0 has a wrong type\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = np.zeros([3, 840,472],dtype=np.uint8)\n",
    "img.fill(255) \n",
    "\n",
    "for i in range(0, 24):\n",
    "    cv2.circle(img, (c_lm[i], c_lm[i+1]), 20, (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (3,) and (3, 840, 472)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ed0cc789d112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openvino/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/anaconda3/envs/openvino/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openvino/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openvino/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             raise ValueError(f\"x and y can be no greater than 2-D, but have \"\n\u001b[0m\u001b[1;32m    403\u001b[0m                              f\"shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (3,) and (3, 840, 472)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ankle_r', -1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"nose\",0-1\n",
    "\"eye_l\",2-3\n",
    "\"eye_r\",4-5\n",
    "\"ear_l\",6-7\n",
    "\"ear_r\",8-9\n",
    "\n",
    "\"shoulder_l\",10-11 y\n",
    "\"shoulder_r\",12-13 y\n",
    "\"elbow_l\",14-15 y\n",
    "\"elbow_r\",16-17 y\n",
    "\"wrist_l\",18-19 y\n",
    "\"wrist_r\",20-21 y\n",
    "\"hip_l\",22-23 y\n",
    "\"hip_r\",24-25 y\n",
    "\"knee_l\",26-27 y\n",
    "\"knee_r\",28-29 y\n",
    "\"ankle_l\",20-31 y\n",
    "\"ankle_r\",32-33 y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11. left_shoulder\n",
    "12. right_shoulder\n",
    "13. left_elbow\n",
    "14. right_elbow\n",
    "15. left_wrist\n",
    "16. right_wrist\n",
    "23. left_hip\n",
    "24. right_hip\n",
    "25. left_knee\n",
    "26. right_knee\n",
    "27. left_ankle\n",
    "28. right_ankle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
