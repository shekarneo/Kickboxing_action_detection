{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7baa49-fc5c-4068-bed6-b571d30a6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# inference\n",
    "model = tf.keras.models.load_model(\"models/pb_model/\")\n",
    "\n",
    "def inference(linkVideo): # This function is used to cycle through a video\n",
    "    X = []\n",
    "    idx = 0\n",
    "    alert_count = 0\n",
    "    video_path = linkVideo  # The path to the data file I use\n",
    "    print(f\"playing : {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    size = (frame_width, frame_height)\n",
    "\n",
    "    # Below VideoWriter object will create\n",
    "    # a frame of above defined The output \n",
    "    # is stored in 'filename.avi' file.\n",
    "    result_writer = cv2.VideoWriter('filename.avi', \n",
    "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                             10, size)\n",
    "    skipTime  = 0\n",
    "    skipFrame = 0\n",
    "    while True:    \n",
    "        ret, frame = cap.read()\n",
    "        skipTime = skipTime +1\n",
    "        if not ret:\n",
    "            break\n",
    "        if 1==1:\n",
    "#         if skipTime >= 30: # When skipTime has passed the first 30 frames, ie the first 1 second, proceed to Detect Person\n",
    "            skipFrame = skipFrame +1 # The variable skipFrame means that every 5 frames I will detect 1 time, so in 1 second I will detect 6 times\n",
    "            # print(skipFrame)\n",
    "#             if  skipFrame  == 5: # When skipFrame = 5, I will detect person and assign skipFrame = 0 to run again\n",
    "            if 1==1:\n",
    "                skipFrame = 0\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame.flags.writeable = False  \n",
    "                result = yolo_model(frame)     # Detect Person\n",
    "                frame.flags.writeable = True   \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                print(frame.shape)\n",
    "                for (xmin, ymin, xmax,   ymax,  confidence,  clas) in result.xyxy[0].tolist(): # Loop through all the Persons present in the video, giving the x,y of each Person\n",
    "                    c_lm = []\n",
    "                    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "                            \n",
    "                            crop_person = frame[int(ymin):int(ymax),int(xmin):int(xmax):]\n",
    "                            resulta = pose.process(crop_person)\n",
    "#                             frame.flags.writeable = True   \n",
    "\n",
    "                            if resulta.pose_landmarks and clas == 0: # class here is class, class = 0 means human\n",
    "                                for (id, lm) in enumerate(resulta.pose_landmarks.landmark):\n",
    "                                    if id > 10 and id not in [17,18,19,20,21,22] and id not in [29,30,31,32] :\n",
    "                                        c_lm.append(lm.x)\n",
    "                                        c_lm.append(lm.y)\n",
    "#                                         c_lm.append(lm.z)\n",
    "#                                         c_lm.append(lm.visibility)\n",
    "                    if len(c_lm) > 0: # c_ lm used to save a person's x and y variables in a loop through each person, when saving, there will be a state that there is no x,y data to save \n",
    "                        X.append(c_lm) # with linkVideo being violent, we add data to X_violent\n",
    "                \n",
    "                    if len(X) >= idx+10:\n",
    "                        X_inp = np.array(X[idx:idx+10])\n",
    "                        pred = model.predict(X_inp.reshape(-1, 10, 24))\n",
    "                        print(pred[0][0])\n",
    "                        if pred[0][0] > 0.50:\n",
    "                            # cv2.putText(frame, str(pred[0][0]), (10, 10), cv2.FONT_HERSHEY_COMPLEX, 2,(255,255,255),3)\n",
    "                            alert_count += 1\n",
    "                        else:\n",
    "                            alert_count = 0\n",
    "                        idx +=1\n",
    "        if alert_count > 5:\n",
    "            cv2.putText(frame, \"Aggression Behaviour Detected!!!\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 2,(0,0,255),3)\n",
    "        \n",
    "        result_writer.write(frame)\n",
    "        cv2.imshow(\"pose\", frame)        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    cap.release()\n",
    "    result_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "video = 0\n",
    "\n",
    "inference(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a61543-1455-442d-a481-9d598c0eb4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3eabf6-2226-476d-a0c1-74863fb05e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdf300-7cb5-45cd-b13a-3abe0daed7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb6d52-0dc8-48bb-b86d-4d05d99c74ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28672671-2610-41c7-b675-db2c7f2d0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with IR detector model\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "import numpy as np\n",
    "from openvino.runtime import Core, get_version\n",
    "\n",
    "sys.path.append('../open_model_zoo-master/demos/common/python')\n",
    "sys.path.append('../open_model_zoo-master/demos/common/python/openvino/model_zoo')\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# inference\n",
    "model = tf.keras.models.load_model(\"models/pb_model/\")\n",
    "\n",
    "def inference(linkVideo): # This function is used to cycle through a video\n",
    "    X = []\n",
    "    idx = 0\n",
    "    alert_count = 0\n",
    "    video_path = linkVideo  # The path to the data file I use\n",
    "    print(f\"playing : {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    size = (frame_width, frame_height)\n",
    "\n",
    "    # Below VideoWriter object will create\n",
    "    # a frame of above defined The output \n",
    "    # is stored in 'filename.avi' file.\n",
    "    result_writer = cv2.VideoWriter('filename.avi', \n",
    "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                             10, size)\n",
    "    skipTime  = 0\n",
    "    skipFrame = 0\n",
    "    while True:    \n",
    "        ret, frame = cap.read()\n",
    "        skipTime = skipTime +1\n",
    "        if not ret:\n",
    "            break\n",
    "        if 1==1:\n",
    "#         if skipTime >= 30: # When skipTime has passed the first 30 frames, ie the first 1 second, proceed to Detect Person\n",
    "            skipFrame = skipFrame +1 # The variable skipFrame means that every 5 frames I will detect 1 time, so in 1 second I will detect 6 times\n",
    "            # print(skipFrame)\n",
    "#             if  skipFrame  == 5: # When skipFrame = 5, I will detect person and assign skipFrame = 0 to run again\n",
    "            if 1==1:\n",
    "                skipFrame = 0\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame.flags.writeable = False  \n",
    "                result = yolo_model(frame)     # Detect Person\n",
    "                frame.flags.writeable = True   \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                print(frame.shape)\n",
    "                for (xmin, ymin, xmax,   ymax,  confidence,  clas) in result.xyxy[0].tolist(): # Loop through all the Persons present in the video, giving the x,y of each Person\n",
    "                    c_lm = []\n",
    "                    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "                            \n",
    "                            crop_person = frame[int(ymin):int(ymax),int(xmin):int(xmax):]\n",
    "                            resulta = pose.process(crop_person)\n",
    "#                             frame.flags.writeable = True   \n",
    "\n",
    "                            if resulta.pose_landmarks and clas == 0: # class here is class, class = 0 means human\n",
    "                                for (id, lm) in enumerate(resulta.pose_landmarks.landmark):\n",
    "                                    if id > 10 and id not in [17,18,19,20,21,22] and id not in [29,30,31,32] :\n",
    "                                        c_lm.append(lm.x)\n",
    "                                        c_lm.append(lm.y)\n",
    "#                                         c_lm.append(lm.z)\n",
    "#                                         c_lm.append(lm.visibility)\n",
    "                    if len(c_lm) > 0: # c_ lm used to save a person's x and y variables in a loop through each person, when saving, there will be a state that there is no x,y data to save \n",
    "                        X.append(c_lm) # with linkVideo being violent, we add data to X_violent\n",
    "                \n",
    "                    if len(X) >= idx+10:\n",
    "                        X_inp = np.array(X[idx:idx+10])\n",
    "                        pred = model.predict(X_inp.reshape(-1, 10, 24))\n",
    "                        print(pred[0][0])\n",
    "                        if pred[0][0] > 0.50:\n",
    "                            # cv2.putText(frame, str(pred[0][0]), (10, 10), cv2.FONT_HERSHEY_COMPLEX, 2,(255,255,255),3)\n",
    "                            alert_count += 1\n",
    "                        else:\n",
    "                            alert_count = 0\n",
    "                        idx +=1\n",
    "        if alert_count > 5:\n",
    "            cv2.putText(frame, \"Aggression Behaviour Detected!!!\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 2,(0,0,255),3)\n",
    "        \n",
    "        result_writer.write(frame)\n",
    "        cv2.imshow(\"pose\", frame)        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    cap.release()\n",
    "    result_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "video = 0\n",
    "\n",
    "inference(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30150a70-623a-43bd-9b39-a54894b780bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
